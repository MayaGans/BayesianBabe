---
title: Dirichlet-multinomial distribution
author: Maya Gans
date: '2019-07-08'
slug: dirichlet-multinomial
categories:
  - Tutorial
tags:
  - ShinyApp
  - Statistics
  - Bayesian
subtitle: ''
summary: ''
authors: []
lastmod: '2019-07-08T10:50:09-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<iframe src="https://maya-gans.shinyapps.io/dir_widget/" height="600px" width="800px">
</iframe>
<p><a href="http://www.statsathome.com/2017/09/14/visualizing-the-multinomial-in-the-simplex/">Inspired By Justin Silverman’s Blog Post</a></p>
<p>My shakey understanding of the very useful Dirichlet Multinomial prior brought me to <a href="https://www.youtube.com/watch?v=7Lbx9q6q8AE">Dr. Brendon Brewer’s lecture</a> explaining how it can be used with an example from the Wheel of Fortune. The contests rotate who spins first by round, but there are 4 rounds. Does this give contestant 1 an unfair advantage? Dr. Brewer tabulated the wins from 30 games to find out.</p>
<div class="figure">
<img src="img1.png" />

</div>
<div id="the-data" class="section level1">
<h1>The Data</h1>
<pre class="r"><code>data &lt;- data.frame(contestant = c(1,2,3), wins = c(8,9,13))
data</code></pre>
<pre><code>##   contestant wins
## 1          1    8
## 2          2    9
## 3          3   13</code></pre>
</div>
<div id="chi-square" class="section level1">
<h1>Chi-Square</h1>
<p>If we were to approach this using a frequentist approach, we would be testing to ensure each contestant has 1/3 of the probability of winning (out of 30 games, each contestant should win 10 times).</p>
<pre class="r"><code>chisq.test(data$wins)</code></pre>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  data$wins
## X-squared = 1.4, df = 2, p-value = 0.4966</code></pre>
<p>We did not supply any <code>given probabilities</code> because the default is to assume equal probability to each contestant. Our p-value here is <code>0.5</code>, we do not have evidence for any unfair advantage among the 3 players from this classical point of view. <strong>But a p-value is not a posterior probability. Also - this test also assumes there are more than 5 occurances in each bin which might not always be the case</strong></p>
<pre class="r"><code>data$wins &gt; 5</code></pre>
<pre><code>## [1] TRUE TRUE TRUE</code></pre>
</div>
<div id="bayesian" class="section level1">
<h1>Bayesian</h1>
<p>We can use a hyper-parameter to say <em>there’s a high chance that the three success probabilities are close together, even if they are not exactly the same</em>.</p>
<p><span class="math display">\[\text{N = 30 trials with } \{\theta_i\} \text{ for going in each bin (winner being contestant 1, 2, or 3)}\]</span></p>
<p><span class="math display">\[\text{Infer } \{\theta_i\} \text{ from the data (get the posterior distribution)}\]</span></p>
<div id="the-sampling-distributionlikelihood-will-be-the-multinomial" class="section level2">
<h2>The sampling distribution/likelihood will be the <strong>multinomial</strong></h2>
<div class="figure">
<img src="img2.png" />

</div>
<p><span class="math display">\[p(x_1,x_2,...,x_n|\theta_1,\theta_2,...,\theta_n) = \frac{N!}{x_1!x_2!...x_n!}\theta^{x_1}_1\theta^{x_2}_2\theta^{x_n}_n\]</span> The probability that all the counts in each category is equal to some outcome given all the success probabilities = the success probabilities to the power of some counts. If we reduce this example to only two outcomes, we would remove the <span class="math inline">\(\theta_n\)</span> part of the equation – <strong>this is the binomial PDF!</strong></p>
</div>
<div id="the-prior-will-be-the-dirichlet" class="section level2">
<h2>The prior will be the <strong>Dirichlet</strong></h2>
<div class="figure">
<img src="img3.png" />

</div>
<p><span class="math display">\[p(\theta_1,...,\theta_n|\alpha_1,...,\alpha_n) \propto \prod^n_{i=1}\theta^{\alpha_i-1}, \sum^n_{i=1}\theta_i=1\]</span></p>
<p>In three dimensions (3 contestants), our constraints put our probability on a triangle where the sum is one. The <span class="math inline">\(\alpha\)</span> parameters control how “diverse” the <span class="math inline">\(\theta\)</span> probabilities are.</p>
</div>
</div>
<div id="playing-with-the-widget-at-the-top-of-the-page" class="section level1">
<h1>Playing with the widget at the top of the page:</h1>
<ul>
<li><p>If we give each <span class="math inline">\(\theta\)</span> equal probability (<span class="math inline">\(\alpha\)</span> = 1/3) we’ll be in the middle of the triangle. You can also see that high <span class="math inline">\(\alpha\)</span>s result in the probability densities lying close to the middle of the simplex; the region where <span class="math inline">\(\theta_1 = \theta_2 = \theta_3 = H_0\)</span>.</p></li>
<li><p>Setting all the <span class="math inline">\(\alpha\)</span>s to 1 will result in a <em>uniform density</em>.</p></li>
<li>Setting low values for the <span class="math inline">\(\alpha\)</span>s gives high probability density in the corners of the triangle.</li>
<li><p>Feel free to check out how sample size effects these densities too!</p></li>
</ul>
</div>
<div id="jags" class="section level1">
<h1>JAGS</h1>
<pre class="r"><code>model = &quot;model
{
  for (i in 1:3) {
    # flat, uniform prior over the simplex
    alpha[i] &lt;- 1
  }
  theta ~ ddirch(alpha)
  x ~ dmulti(theta, N)
}
&quot;

# The data (use NA for no data)
data = list(x=c(8,9,13), N = 30)

# Variables to monitor
variable_names = c(&#39;theta&#39;)

# How many burn-in steps?
burn_in = 1000

# How many proper steps?
steps = 10000

# Thinning?
thin = 1

# Random number seed
seed = 42


# NO NEED TO EDIT PAST HERE!!!
# Just run it all and use the results list.

library(rjags)</code></pre>
<pre><code>## Loading required package: coda</code></pre>
<pre><code>## Linked to JAGS 4.3.0</code></pre>
<pre><code>## Loaded modules: basemod,bugs</code></pre>
<pre class="r"><code># Write model out to file
fileConn=file(&quot;model.temp&quot;)
writeLines(model, fileConn)
close(fileConn)

if(all(is.na(data)))
{
    m = jags.model(file=&quot;model.temp&quot;, inits=list(.RNG.seed=seed, .RNG.name=&quot;base::Mersenne-Twister&quot;))
} else
{
    m = jags.model(file=&quot;model.temp&quot;, data=data, inits=list(.RNG.seed=seed, .RNG.name=&quot;base::Mersenne-Twister&quot;))
}</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 1
##    Unobserved stochastic nodes: 1
##    Total graph size: 5
## 
## Initializing model</code></pre>
<pre class="r"><code>update(m, burn_in)
draw = jags.samples(m, steps, thin=thin, variable.names = variable_names)
# Convert to a list
make_list &lt;- function(draw)
{
    results = list()
    for(name in names(draw))
    {
        # Extract &quot;chain 1&quot;
        results[[name]] = as.array(draw[[name]][,,1])
        
        # Transpose 2D arrays
        if(length(dim(results[[name]])) == 2)
            results[[name]] = t(results[[name]])
    }
    return(results)
}
results = make_list(draw)

plot(results$theta[,1], type = &quot;l&quot;)</code></pre>
<p><img src="/post/2019-07-08-multinomial-dirichlet/index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>This is the success probability of the person in position one, and it looks like their probability is around 1/3. We get similar results for the other two players.</p>
<pre class="r"><code>mean(results$theta[,1])</code></pre>
<pre><code>## [1] 0.2725863</code></pre>
<pre class="r"><code>mean(results$theta[,2])</code></pre>
<pre><code>## [1] 0.3038159</code></pre>
<pre class="r"><code>mean(results$theta[,3])</code></pre>
<pre><code>## [1] 0.4235978</code></pre>
<p>The third probability is the highest, but that’s only because more data went into the third probability than the second, and more in the second than the third. Looking at our original hypothesis:</p>
<pre class="r"><code>mean(results$theta[,3] &gt; mean(results$theta[,1]))</code></pre>
<pre><code>## [1] 0.9647</code></pre>
<p>It turns out we were wrong, and contestant three actually has the advantage, not contestant 1.</p>
<pre class="r"><code>sd(results$theta[,3])</code></pre>
<pre><code>## [1] 0.08462054</code></pre>
<p>However, the posterior SD is relatively big because these point estimates are not the most reliable.</p>
</div>
<div id="hierarchical" class="section level1">
<h1>Hierarchical</h1>
<p>We can also make the <span class="math inline">\(\alpha\)</span>s an unknown (with the possibility that it’s high) rather than assume a particular alpha:</p>
<pre class="r"><code>model = &quot;model
{
  # alpha can possibly be high (probablities close together)
  # log uniform prior

  # a = e^-1 (probabilites in corners) to e^5 (probabilities in the center)
  log_a ~ dunif(-1,5)

  # set all alphas to be a
  a &lt;- exp(log_a)

  for (i in 1:3) {
    # flat, uniform prior over the simplex
    alpha[i] &lt;- a
  }
  theta ~ ddirch(alpha)
  x ~ dmulti(theta, N)
}
&quot;

# The data (use NA for no data)
data = list(x=c(8,9,13), N = 30)

# Variables to monitor
variable_names = c(&#39;theta&#39;)

# How many burn-in steps?
burn_in = 1000

# How many proper steps?
steps = 10000

# Thinning?
thin = 1

# Random number seed
seed = 42


# NO NEED TO EDIT PAST HERE!!!
# Just run it all and use the results list.

library(rjags)

# Write model out to file
fileConn=file(&quot;model.temp&quot;)
writeLines(model, fileConn)
close(fileConn)

if(all(is.na(data)))
{
    m = jags.model(file=&quot;model.temp&quot;, inits=list(.RNG.seed=seed, .RNG.name=&quot;base::Mersenne-Twister&quot;))
} else
{
    m = jags.model(file=&quot;model.temp&quot;, data=data, inits=list(.RNG.seed=seed, .RNG.name=&quot;base::Mersenne-Twister&quot;))
}</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 1
##    Unobserved stochastic nodes: 2
##    Total graph size: 9
## 
## Initializing model</code></pre>
<pre class="r"><code>update(m, burn_in)
draw = jags.samples(m, steps, thin=thin, variable.names = variable_names)
# Convert to a list
make_list &lt;- function(draw)
{
    results = list()
    for(name in names(draw))
    {
        # Extract &quot;chain 1&quot;
        results[[name]] = as.array(draw[[name]][,,1])
        
        # Transpose 2D arrays
        if(length(dim(results[[name]])) == 2)
            results[[name]] = t(results[[name]])
    }
    return(results)
}
results = make_list(draw)

# trace plot looks good
plot(results$theta[,1], type = &quot;l&quot;)</code></pre>
<p><img src="/post/2019-07-08-multinomial-dirichlet/index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The prior put more plausibility on the statement that the three posteriors are closer together. Our uncertainty is also lower because we’ve used an informative prior!</p>
<pre class="r"><code>mean(results$theta[,1])</code></pre>
<pre><code>## [1] 0.3087914</code></pre>
<pre class="r"><code>mean(results$theta[,2])</code></pre>
<pre><code>## [1] 0.3222356</code></pre>
<pre class="r"><code>mean(results$theta[,3])</code></pre>
<pre><code>## [1] 0.368973</code></pre>
<pre class="r"><code>sd(results$theta[,1])</code></pre>
<pre><code>## [1] 0.05259085</code></pre>
<pre class="r"><code>sd(results$theta[,2])</code></pre>
<pre><code>## [1] 0.05118229</code></pre>
<pre class="r"><code>sd(results$theta[,3])</code></pre>
<pre><code>## [1] 0.05911318</code></pre>
<p>Using an informative prior we also see that the player in the third position has a higher advantage (albeit a small one)</p>
<pre class="r"><code>mean(results$theta[,3] &gt; mean(results$theta[,1]))</code></pre>
<pre><code>## [1] 0.8886</code></pre>
<p>Because of the constraint that our <span class="math inline">\(\theta\)</span>s must add up to 1, there is a correlation in our prior, which also shows up in the posterior (the two are anti-correlated because they need to add up to 1).</p>
<pre class="r"><code>plot(results$theta[,1], results$theta[,3], cex = 0.1)</code></pre>
<p><img src="/post/2019-07-08-multinomial-dirichlet/index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
